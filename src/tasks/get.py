import concurrent.futures
import tempfile
from collections.abc import Iterator
from functools import partial
from pathlib import Path
from time import sleep

import httpx
import ijson
import orjson
import polars as pl
from httpx import Client, HTTPStatusError, TimeoutException, get
from lxml import etree, html
from prefect import task
from prefect.transactions import transaction

from config import SIRENE_UNITES_LEGALES_URL
from src.config import (
    DATA_DIR,
    DECP_PROCESSING_PUBLISH,
    DECP_USE_CACHE,
    HTTP_CLIENT,
    HTTP_HEADERS,
    RESOURCE_CACHE_DIR,
    DecpFormat,
)
from src.schemas import SCHEMA_MARCHE_2019, SCHEMA_MARCHE_2022
from src.tasks.clean import (
    clean_decp,
    clean_invalid_characters,
    extract_innermost_struct,
)
from src.tasks.output import sink_to_files
from src.tasks.utils import (
    full_resource_name,
    gen_artifact_row,
    stream_replace_bytestring,
)
from tasks.transform import prepare_unites_legales


@task(retries=3, retry_delay_seconds=3)
def stream_get(url: str, chunk_size=1024**2):  # chunk_size en octets (1 Mo par d√©faut)
    if url.startswith("http"):
        try:
            with HTTP_CLIENT.stream(
                "GET", url, headers=HTTP_HEADERS, follow_redirects=True
            ) as response:
                yield from response.iter_bytes(chunk_size)
        except httpx.TooManyRedirects:
            print(f"‚õîÔ∏è Erreur 429 Too Many Requests pour {url}")
            return

    else:
        # Donn√©es de test.
        with open(url, "rb") as f:
            for chunk in iter(partial(f.read, chunk_size), b""):
                yield chunk


@task(persist_result=False)
def get_resource(
    r: dict, resources_artifact: list[dict] | list
) -> tuple[pl.LazyFrame | None, DecpFormat | None]:
    if DECP_PROCESSING_PUBLISH is False:
        print(f"‚û°Ô∏è  {full_resource_name(r)}")

    output_path = DATA_DIR / "get" / r["filename"]
    output_path.parent.mkdir(exist_ok=True, parents=True)
    url = r["url"]
    file_format = r["format"]
    if file_format == "json":
        fields, decp_format = json_stream_to_parquet(url, output_path, r)
    elif file_format == "xml":
        try:
            fields, decp_format = xml_stream_to_parquet(
                url, output_path, fix_chars=False
            )
        except etree.XMLSyntaxError:
            fields, decp_format = xml_stream_to_parquet(
                url, output_path, fix_chars=True
            )
            print(f"‚ôªÔ∏è  {full_resource_name(r)} nettoy√© et trait√©")
    else:
        print(f"‚ñ∂Ô∏è  Format de fichier non support√© : {full_resource_name(r)}")
        return None, None

    if decp_format is None:
        return None, None

    lf: pl.LazyFrame = pl.scan_parquet(output_path.with_suffix(".parquet"))

    # Ajout des stats de la ressource √† l'artifact
    # https://github.com/ColinMaudry/decp-processing/issues/89
    if DECP_PROCESSING_PUBLISH:
        artifact_row = gen_artifact_row(r, lf, url, fields, decp_format)  # noqa
        resources_artifact.append(artifact_row)

    # Exemple https://www.data.gouv.fr/datasets/5cd57bf68b4c4179299eb0e9/#/resources/bb90091c-f0cb-4a59-ad41-b0ab929aad93
    resource_web_url = (
        f"https://www.data.gouv.fr/datasets/{r['dataset_id']}/#/resources/{r['id']}"
    )

    lf = lf.with_columns(pl.lit(resource_web_url).alias("sourceFile"))

    if r["dataset_code"] == "decp_minef":
        lf = lf.with_columns(
            (pl.lit("decp_minef_") + pl.col("source")).alias("sourceDataset")
        )
        lf = lf.drop("source")
    else:
        lf = lf.rename({"source": "sourceDataset"})
        lf = lf.with_columns(pl.lit(r["dataset_code"]).alias("sourceDataset"))

    return lf, decp_format


def find_json_decp_format(chunk, decp_formats, resource: dict):
    for decp_format in decp_formats:
        decp_format.coroutine_ijson.send(chunk)
        if len(decp_format.liste_marches_ijson) > 0:
            # Le parser a trouv√© au moins un march√© correspondant √† ce format, donc on a
            # trouv√© le bon format.
            return decp_format
    print(
        f"‚ö†Ô∏è  Pas de match trouv√© parmis les sch√©mas pass√©s : {full_resource_name(resource)}"
    )
    return None


@task(persist_result=False, log_prints=True)
def json_stream_to_parquet(
    url: str, output_path: Path, resource: dict
) -> tuple[set, DecpFormat or None]:
    decp_format_2019 = DecpFormat("DECP 2019", SCHEMA_MARCHE_2019, "marches")
    decp_format_2022 = DecpFormat("DECP 2022", SCHEMA_MARCHE_2022, "marches.marche")
    decp_formats = [decp_format_2019, decp_format_2022]

    fields = set()
    for decp_format in decp_formats:
        decp_format.liste_marches_ijson = ijson.sendable_list()
        decp_format.coroutine_ijson = ijson.items_coro(
            decp_format.liste_marches_ijson,
            f"{decp_format.prefixe_json_marches}.item",
            use_float=True,
        )

    tmp_file = tempfile.NamedTemporaryFile(mode="wb", suffix=".ndjson", delete=False)

    http_stream_iter = stream_get(url)

    stream_replace_iter = stream_replace_bytestring(
        http_stream_iter, rb"NaN([,\n])", rb"null\1"
    )  # Nan => null

    # Le dataset AWS scraping a pas mal de bugs de backslash
    if "/68caf6b135f19236a4f37a32/" in url or "/aws/" in url:
        print("Remplacements sp√©cifiques pour AWS...")
        stream_replace_iter = stream_replace_bytestring(
            stream_replace_bytestring(
                stream_replace_bytestring(stream_replace_iter, rb"(\\\\\\)", rb"\\"),
                rb"\\\\",
                rb"\\",
            ),
            rb"\\ ",
            rb" ",
        )

    # In first iteration, will find the right format
    try:
        chunk = next(stream_replace_iter)
    except StopIteration:
        print(f"‚ö†Ô∏è  Flux vide pour {url}")
        return set(), None

    decp_format = find_json_decp_format(chunk, decp_formats, resource)
    if decp_format is None:
        return set(), None

    for marche in decp_format.liste_marches_ijson:
        new_fields = write_marche_rows(marche, tmp_file, decp_format)
        fields = fields.union(new_fields)

    del decp_format.liste_marches_ijson[:]

    for chunk in stream_replace_iter:
        decp_format.coroutine_ijson.send(chunk)
        for marche in decp_format.liste_marches_ijson:
            new_fields = write_marche_rows(marche, tmp_file, decp_format)
            fields = fields.union(new_fields)

        del decp_format.liste_marches_ijson[:]

    decp_format.coroutine_ijson.close()
    tmp_file.seek(0)

    lf = pl.scan_ndjson(tmp_file.name, schema=decp_format.schema)
    sink_to_files(lf, output_path, file_format="parquet")

    tmp_file.close()

    return fields, decp_format


@task(persist_result=False)
def xml_stream_to_parquet(
    url: str, output_path: Path, fix_chars=False
) -> tuple[set, DecpFormat]:
    """Uniquement utilis√© pour les donn√©es publi√©es par l'AIFE."""

    decp_format_2022 = DecpFormat("DECP 2022", SCHEMA_MARCHE_2022, "marches.marche")

    fields = set()
    parser = etree.XMLPullParser(tag="marche", recover=True)
    with tempfile.NamedTemporaryFile(
        mode="wb", suffix=".ndjson", delete=True
    ) as tmp_file:
        for chunk in stream_get(url):
            if fix_chars:
                chunk = clean_invalid_characters(chunk)
            parser.feed(chunk)
            for _, elem in parser.read_events():
                marche = parse_element(elem)
                new_fields = write_marche_rows(marche, tmp_file, decp_format_2022)
                fields = fields.union(new_fields)
        lf = pl.scan_ndjson(tmp_file.name, schema=decp_format_2022.schema)
        sink_to_files(lf, output_path, file_format="parquet")
    return fields, decp_format_2022


# G√©n√©r√©e par la LLM Euria, d√©velopp√©e par Infomaniak
def parse_element(elem):
    """
    Parse un √©l√©ment XML en dictionnaire Python.
    Pour les tags comme <modalitesExecution>, <considerationsSociales>, etc.,
    on conserve la structure : {"modaliteExecution": [...]} au lieu de [...] (format 2022)
    """

    # Si l'√©l√©ment n'a ni enfants ni texte ‚Üí retourne None
    if len(elem) == 0 and not elem.text:
        return None

    # Si l'√©l√©ment n'a pas d'enfants ‚Üí retourne son texte (nettoy√©)
    if len(elem) == 0:
        return elem.text.strip() if elem.text else ""

    # Collecte les enfants sous forme de listes (m√™me si un seul enfant)
    children = {}
    for child in elem:
        tag = child.tag
        if tag not in children:
            children[tag] = []
        children[tag].append(parse_element(child))

    # Cas sp√©ciaux : ces √©l√©ments doivent toujours √™tre des objets avec une cl√© liste
    if elem.tag == "titulaires":
        # Chaque <titulaire> devient un objet dans une liste
        return [{"titulaire": item} for item in children.get("titulaire", [])]

    # Pour les tags comme <considerationsSociales>, <modalitesExecution>, etc.
    # on utilise le premier tag enfant existant
    elif elem.tag in [
        "considerationsSociales",
        "considerationsEnvironnementales",
        "techniques",
        "modalitesExecution",
        "typesPrix",
    ]:
        # On r√©cup√®re le premier tag enfant (s‚Äôil existe)
        if children:
            # On prend le premier tag enfant comme cl√©
            first_child_tag = next(iter(children.keys()))
            # On retourne un objet avec cette cl√© ‚Üí valeur = liste des enfants
            return {first_child_tag: children[first_child_tag]}
        else:
            # Si pas d'enfant ‚Üí retourne un objet vide
            return {next(iter(children.keys())) if children else "": []}

    # Pour tous les autres √©l√©ments : si un seul enfant ‚Üí valeur simple, sinon liste
    result = {}
    for tag, values in children.items():
        if len(values) == 1:
            result[tag] = values[0]  # Pas de liste si un seul √©l√©ment
        else:
            result[tag] = values  # Sinon, on garde la liste

    return result


def write_marche_rows(marche: dict, file, decp_format: DecpFormat) -> set[str]:
    """Ajout d'une ligne ndjson pour chaque modification/version du march√©."""
    fields = set()
    if marche:  # marche peut √™tre null (marches-securises.fr)
        for mod in yield_modifications(marche):
            if mod is None:
                continue
            # Pour decp-2019.json : d√©simbrication des donn√©es des titulaires
            # voir https://github.com/ColinMaudry/decp-processing/issues/114
            # compl√®te probablement norm_titulaires(), qui ne faisait pas compl√®tement le taff, donc √† fusionner
            if decp_format.label == "DECP 2019":
                for f in ["titulaires", "modification_titulaires"]:
                    liste_titulaires = mod.get(f)
                    if liste_titulaires and isinstance(liste_titulaires[0], list):
                        mod[f] = extract_innermost_struct(liste_titulaires)
            file.write(orjson.dumps(mod))
            file.write(b"\n")
            fields = fields.union(mod.keys())
    return fields


def yield_modifications(row: dict, separator="_") -> Iterator[dict] or None:
    """Pour chaque modification, g√©n√®re un objet/dict march√© aplati."""
    raw_mods = row.pop("modifications", [])
    # Couvre le format 2022:
    if isinstance(raw_mods, dict) and "modification" in raw_mods:
        raw_mods = raw_mods["modification"]
    # Couvre le (non-)format dans lequel "modifications" ou "modification" m√®ne
    # directement √† un dict contenant les m√©tadonn√©es li√©es √† une modification.
    if isinstance(raw_mods, dict):
        raw_mods = [raw_mods]
    elif isinstance(raw_mods, str) or raw_mods is None:
        raw_mods = []

    mods = [{}] + raw_mods
    for i, mod in enumerate(mods):
        mod["id"] = i
        if "modification" in mod:
            mod = mod["modification"]
        titulaires = norm_titulaires(mod)
        if titulaires is not None:
            mod["titulaires"] = titulaires
        row["modification"] = mod
        yield pl.convert.normalize._simple_json_normalize(
            row, separator, 10, lambda x: x
        )


def norm_titulaires(titulaires):
    """
    Corrige les blocs titulaires imbriqu√©s dans n niveaux de listes.

    :param titulaires:
    :return: titulaires:
    """
    if isinstance(titulaires, list):
        titulaires_clean = []
        for t in titulaires:
            if isinstance(t, dict):
                titulaires_clean.append(norm_titulaire(t))
            elif isinstance(t, list):
                # Traite les listes de titulaires √©crites en listes de listes.
                for inner_t in t:
                    if isinstance(inner_t, dict):
                        titulaires_clean.append(norm_titulaire(inner_t))
        return titulaires_clean
    return None


# R√©cup√©ration des donn√©es des √©tablissements
def norm_titulaire(titulaire: dict):
    if "titulaire" in titulaire:
        titulaire = titulaire["titulaire"]
    return titulaire


def get_etablissements() -> pl.LazyFrame:
    schema = {
        "siret": pl.String,
        "codeCommuneEtablissement": pl.String,
        "latitude": pl.Float64,
        "longitude": pl.Float64,
        "activitePrincipaleEtablissement": pl.String,
        "nomenclatureActivitePrincipaleEtablissement": pl.String,
        "enseigne1Etablissement": pl.String,
        "denominationUsuelleEtablissement": pl.String,
    }

    columns = list(schema.keys())
    print(columns)

    base_url = "https://files.data.gouv.fr/geo-sirene/last/dep/"
    htmlpage: str = get(base_url).text
    htmlpage: html.HtmlElement = html.fromstring(htmlpage)
    http_client = Client()

    # Pr√©paration des hrefs
    hrefs = []
    for link in htmlpage.findall(".//a"):
        href = link.get("href")
        if href.startswith("geo_siret"):
            hrefs.append(base_url + href)

    # Fonction de traitement pour un fichier
    def get_process_file(_href: str):
        print(_href.split("/")[-1])
        try:
            response = http_client.get(
                _href, headers=HTTP_HEADERS, timeout=20
            ).raise_for_status()
        except (HTTPStatusError, TimeoutException) as err:
            print(err)
            print("Nouvel essai...")
            response = http_client.get(
                _href, headers=HTTP_HEADERS, timeout=20
            ).raise_for_status()

        content = response.content
        lff = pl.scan_csv(content, schema_overrides=schema)
        lff = lff.select(columns)
        return lff

    # Traitement en parrall√®le avec 8 threads
    lfs = []
    with concurrent.futures.ThreadPoolExecutor(max_workers=8) as executor:
        futures = [executor.submit(get_process_file, href) for href in hrefs]
        for future in concurrent.futures.as_completed(futures):
            try:
                lf = future.result()
                lfs.append(lf)
            except Exception as e:
                print(f"Error processing file: {e}")

    print("Concat√©nation...")
    lf_etablissements: pl.LazyFrame = pl.concat(lfs)
    return lf_etablissements


def get_insee_cog_data(url, schema_overrides, columns) -> pl.DataFrame:
    try:
        df_insee = pl.read_csv(url, schema_overrides=schema_overrides, columns=columns)
    except ConnectionResetError:
        print("Connection error, retrying in 2 seconds...")
        sleep(2)
        df_insee = get_insee_cog_data(
            url, schema_overrides=schema_overrides, columns=columns
        )
    return df_insee


@task(
    log_prints=True,
    # persist_result=True,
    # cache_expiration=datetime.timedelta(hours=CACHE_EXPIRATION_TIME_HOURS),
    # cache_key_fn=get_clean_cache_key,
)
def get_clean(
    resource, resources_artifact: list, available_parquet_files: set
) -> pl.DataFrame or None:
    with transaction():
        checksum = resource["checksum"]
        parquet_path = RESOURCE_CACHE_DIR / f"{checksum}"

        # Si la ressource n'est pas en cache ou que l'utilisation du cache est d√©sactiv√©e
        if (
            DECP_USE_CACHE is False
            or f"{checksum}.parquet" not in available_parquet_files
        ):
            # R√©cup√©ration des donn√©es source...
            lf, decp_format = get_resource(resource, resources_artifact)

            # Nettoyage des donn√©es source et typage des colonnes...
            # si la ressource est dans un format support√©
            if lf is not None and not Path(parquet_path).exists():
                lf: pl.LazyFrame = clean_decp(lf, decp_format)
                sink_to_files(
                    lf, parquet_path, file_format="parquet", compression="zstd"
                )
                return parquet_path.with_suffix(".parquet")
            else:
                return None
        else:
            # Le fichier parquet est d√©j√† disponible pour ce checksum
            print(f"üëç Ressource d√©j√† en cache : {resource['dataset_code']}")
            return parquet_path.with_suffix(".parquet")


@task
def get_unite_legales(processed_parquet_path):
    print("T√©l√©chargement des donn√©es unit√© l√©gales et s√©lection des colonnes...")
    (
        pl.scan_parquet(SIRENE_UNITES_LEGALES_URL)
        .pipe(prepare_unites_legales)
        .sink_parquet(processed_parquet_path)
    )
