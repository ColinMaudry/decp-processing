import datetime
import os
import shutil

import polars as pl
from prefect import flow, task
from prefect.task_runners import ConcurrentTaskRunner
from prefect.transactions import transaction

from config import (
    BASE_DF_COLUMNS,
    CACHE_EXPIRATION_TIME_HOURS,
    DATE_NOW,
    DECP_PROCESSING_PUBLISH,
    DIST_DIR,
    MAX_PREFECT_WORKERS,
    SIRENE_DATA_DIR,
    TRACKED_DATASETS,
)
from tasks.clean import clean_decp
from tasks.dataset_utils import list_resources
from tasks.enrich import enrich_from_sirene
from tasks.get import get_resource
from tasks.output import (
    save_to_files,
    save_to_sqlite,
)
from tasks.publish import publish_to_datagouv
from tasks.transform import (
    concat_decp_json,
    get_prepare_unites_legales,
    normalize_tables,
    sort_columns,
)
from tasks.utils import create_sirene_data_dir, generate_stats, remove_unused_cache


def get_clean_cache_key(context, parameters) -> str:
    resource = parameters["resource"]
    # TOOD d√©placer cette fonction vers tasks/utils.py

    # On utilise le hash sha1 de la ressource, g√©n√©r√© par data.gouv.fr, comme cl√© de cache
    return resource["checksum"]


@task(
    log_prints=True,
    persist_result=True,
    cache_expiration=datetime.timedelta(hours=CACHE_EXPIRATION_TIME_HOURS),
    cache_key_fn=get_clean_cache_key,
)
def get_clean(resource) -> pl.DataFrame or None:
    # R√©cup√©ration des donn√©es source...
    with transaction():
        lf: pl.LazyFrame = get_resource(resource)
        df = None

        # Nettoyage des donn√©es source et typage des colonnes...
        # si la ressource est dans un format support√©
        if lf is not None:
            lf = clean_decp(lf)
            df = lf.collect(engine="streaming")

    return df


@task(log_prints=True)
def make_datalab_data():
    """T√¢ches consacr√©es √† la transformation des donn√©es dans un format
    adapt√© aux activit√©s du Datalab d'Anticor."""

    print("Cr√©ation de la base donn√©es pour le Datalab d'Anticor...")

    df: pl.DataFrame = pl.read_parquet(DIST_DIR / "decp.parquet")

    print("Enregistrement des DECP (base DataFrame) au format SQLite...")
    save_to_sqlite(
        df,
        "datalab",
        "data.gouv.fr.2022.clean",
        "uid, titulaire_id, titulaire_typeIdentifiant, modification_id",
    )

    print("Normalisation des tables...")
    normalize_tables(df)

    if DECP_PROCESSING_PUBLISH.lower() == "true":
        print("Publication sur data.gouv.fr...")
        publish_to_datagouv(context="datalab")
    else:
        print("Publication sur data.gouv.fr d√©sactiv√©e.")


@flow(
    log_prints=True, task_runner=ConcurrentTaskRunner(max_workers=MAX_PREFECT_WORKERS)
)
def decp_processing(enable_cache_removal: bool = False):
    print("üöÄ  D√©but du flow decp-processing")

    print("Liste de toutes les ressources des datasets...")
    resources: list[dict] = list_resources(TRACKED_DATASETS)

    # Traitement parall√®le des ressources
    futures = [get_clean.submit(resource) for resource in resources]
    dfs: list[pl.DataFrame] = [f.result() for f in futures if f.result() is not None]

    print("Fusion des dataframes...")
    df: pl.DataFrame = concat_decp_json(dfs)

    print("Ajout des donn√©es SIRENE...")
    # Preprocessing des donn√©es SIRENE si :
    # - le dossier n'existe pas encore (= les donn√©es n'ont pas d√©j√† √©t√© preprocessed ce mois-ci)
    # - on est au moins le 5 du mois (pour √™tre s√ªr que les donn√©es SIRENE ont √©t√© mises √† jour sur data.gouv.fr)
    if not SIRENE_DATA_DIR.exists() and int(DATE_NOW[-2:]) >= 5:
        sirene_preprocess()
    lf: pl.LazyFrame = enrich_from_sirene(df.lazy())

    print("G√©n√©ration de l'artefact (statistiques) sur le base df...")
    df: pl.DataFrame = lf.collect(engine="streaming")

    generate_stats(df)

    if os.path.exists(DIST_DIR):
        shutil.rmtree(DIST_DIR)
    os.makedirs(DIST_DIR)

    print("Enregistrement des DECP aux formats CSV, Parquet...")
    df: pl.DataFrame = sort_columns(df, BASE_DF_COLUMNS)
    save_to_files(df, DIST_DIR / "decp")

    # Base de donn√©es SQLite d√©di√©e aux activit√©s du Datalab d'Anticor
    make_datalab_data()

    # Suppression des fichiers de cache inutilis√©s
    if enable_cache_removal:
        remove_unused_cache()

    print("‚òëÔ∏è  Fin du flow principal decp_processing.")


@flow(log_prints=True)
def sirene_preprocess():
    """Pr√©traitement mensuel des donn√©es SIRENE afin d'√©conomiser du temps lors du traitement quotidien des DECP.
    Pour chaque ressource (unit√©s l√©gales, √©tablissements), un fichier parquet est produit.
    """

    print("üöÄ  Pr√©-traitement des donn√©es SIRENE")
    # Soit les t√¢ches de ce flow vont au bout (success), soit le dossier SIRENE_DATA_DIR est supprim√© (voir remove_sirene_data_dir())
    with transaction():
        create_sirene_data_dir()

        # TODO pr√©parer lest donn√©es √©tablissements

        # pr√©parer les donn√©es unit√©s l√©gales
        processed_parquet_path = SIRENE_DATA_DIR / "unites_legales.parquet"
        if not processed_parquet_path.exists():
            print("Pr√©pararion des unit√©s l√©gales...")
            get_prepare_unites_legales(processed_parquet_path)

    print("‚òëÔ∏è  Fin du flow sirene_preprocess.")


if __name__ == "__main__":
    decp_processing()
